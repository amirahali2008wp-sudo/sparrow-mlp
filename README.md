# Sparrow ğŸ¦: A Dynamic MLP Architecture

[![PyPI version](https://badge.fury.io/py/sparrow-mlp.svg)](https://badge.fury.io/py/sparrow-mlp)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**[English](#english) | [ÙØ§Ø±Ø³ÛŒ](#ÙØ§Ø±Ø³ÛŒ)**

---

## English

Sparrow is a PyTorch library for building **Dynamic Multi-Layer Perceptrons (MLPs)**. This architecture learns its own optimal structure by combining two powerful routing mechanisms:

1.  **Dynamic Depth (Global Routing)**: A top-level router learns to activate or bypass entire hidden layers, finding the most efficient computational path for each input.
2.  **Mixture of Experts (Local Routing)**: Each hidden layer is a MoE layer containing several smaller "expert" networks. A sophisticated **Two-Tower Router** within each layer selects the most relevant expert, enabling specialization and efficient, sparse computation.

This results in a highly adaptive neural network that prunes its own depth and width during training, controlled by intuitive hyperparameters for sparsity, load balancing, and exploration.

### Key Features
-   **Dynamic Depth**: Automatically learns which layers to skip.
-   **Mixture of Experts Layers**: Activates a single, specialized group of neurons in each layer.
-   **Two-Tower Routers**: An advanced routing mechanism for intelligent expert selection.
-   **Built-in Optimizations**: Optional support for load balancing and entropy bonus to ensure robust training.
-   **Simple API**: Build and configure complex dynamic models easily with the `SparrowConfig` class.

### Installation
```bash
pip install sparrow-mlp
```

### Quickstart
Here is a complete example of training a `DynamicMLP` on the Iris dataset.
```python
# examples/run_iris.py

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ sparrow Ù†ØµØ¨ Ø´Ø¯Ù‡ Ùˆ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø§Ø³Øª
from sparrow import SparrowConfig, DynamicMLP

# Û±. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_train_tensor = torch.FloatTensor(X_train)
y_train_tensor = torch.LongTensor(y_train)
X_test_tensor = torch.FloatTensor(X_test)
y_test_tensor = torch.LongTensor(y_test)

# Û². ØªØ¹Ø±ÛŒÙ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² SparrowConfig
config = SparrowConfig(
    hidden_dim=32,
    num_hidden_layers=2,
    num_experts=4,
    expert_hidden_size=128,
    layer_sparsity_lambda=0.01,
    load_balancing_alpha=0.01,
    entropy_lambda=0.0  # Ù¾Ø§Ø¯Ø§Ø´ Ø§Ù†ØªØ±ÙˆÙ¾ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù…Ø«Ø§Ù„ Ø³Ø§Ø¯Ù‡ ØºÛŒØ±ÙØ¹Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
)

# Û³. Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ø¨Ø§ Ù¾Ø§Ø³ Ø¯Ø§Ø¯Ù† Ú©Ø§Ù†ÙÛŒÚ¯
model = DynamicMLP(input_size=4, output_size=3, config=config)
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()
epochs = 500

# Û´. Ø­Ù„Ù‚Ù‡ Ø¢Ù…ÙˆØ²Ø´
print("--- Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ DynamicMLP Ø±ÙˆÛŒ Ø¯ÛŒØªØ§Ø³Øª Ø²Ù†Ø¨Ù‚ ---")
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    
    # Ù…Ø¯Ù„ Ø¯Ø± Ø­Ø§Ù„Øª Ø¢Ù…ÙˆØ²Ø´ Ø³Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ø§Ø±Ø¯
    outputs, balancing_loss, entropy = model(X_train_tensor)
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªÙ…Ø§Ù… Ù…Ø¤Ù„ÙÙ‡â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ§Ù†
    loss_cls = criterion(outputs, y_train_tensor)
    loss_sparsity = config.layer_sparsity_lambda * model.layer_gates_values.sum()
    loss_balance = config.load_balancing_alpha * balancing_loss
    bonus_entropy = config.entropy_lambda * entropy

    total_loss = loss_cls + loss_sparsity + loss_balance - bonus_entropy
    total_loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Total Loss: {total_loss.item():.4f}')

# Ûµ. ØªØ³Øª Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„
model.eval()
with torch.no_grad():
    test_outputs = model(X_test_tensor)
    _, predicted_labels = torch.max(test_outputs, 1)
    accuracy = accuracy_score(y_test_tensor.numpy(), predicted_labels.numpy())
    print(f'\nØ¯Ù‚Øª Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª: {accuracy * 100:.2f}%')
```
---

## ÙØ§Ø±Ø³ÛŒ

`Sparrow` ğŸ¦ ÛŒÚ© Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ù¾Ø§ÛŒØªÙˆØ±Ú† Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª **Ù¾Ø±Ø³Ù¾ØªØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ù„Ø§ÛŒÙ‡ Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© (MLP)** Ø§Ø³Øª. Ø§ÛŒÙ† Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø³Ø§Ø®ØªØ§Ø± Ø¨Ù‡ÛŒÙ†Ù‡ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ ØªØ±Ú©ÛŒØ¨ Ø¯Ùˆ Ù…Ú©Ø§Ù†ÛŒØ²Ù… Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ÛŒ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯:

Û±. **Ø¹Ù…Ù‚ Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© (Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ÛŒ Ø³Ø±Ø§Ø³Ø±ÛŒ)**: ÛŒÚ© Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ Ø³Ø·Ø­ Ø¨Ø§Ù„Ø§ ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ú©Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆØ±ÙˆØ¯ÛŒØŒ Ú©Ù„ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù† Ø±Ø§ ÙØ¹Ø§Ù„ ÛŒØ§ Ø§Ø² Ø¢Ù†Ù‡Ø§ Ø¹Ø¨ÙˆØ± Ú©Ù†Ø¯ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØªØ±ÛŒÙ† Ù…Ø³ÛŒØ± Ù…Ø­Ø§Ø³Ø¨Ø§ØªÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆØ±ÙˆØ¯ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ù†Ø¯.
Û². **ØªØ±Ú©ÛŒØ¨ÛŒ Ø§Ø² Ù…ØªØ®ØµØµØ§Ù† (Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ÛŒ Ù…Ø­Ù„ÛŒ)**: Ù‡Ø± Ù„Ø§ÛŒÙ‡ Ù¾Ù†Ù‡Ø§Ù† ÛŒÚ© Ù„Ø§ÛŒÙ‡ MoE Ø§Ø³Øª Ú©Ù‡ Ø´Ø§Ù…Ù„ Ú†Ù†Ø¯ÛŒÙ† Ø´Ø¨Ú©Ù‡ "Ù…ØªØ®ØµØµ" Ú©ÙˆÚ†Ú©ØªØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯. ÛŒÚ© **Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ Ø¯Ùˆ Ø¨Ø±Ø¬ÛŒ (Two-Tower Router)** Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¯Ø± Ù‡Ø± Ù„Ø§ÛŒÙ‡ØŒ Ù…Ø±ØªØ¨Ø·â€ŒØªØ±ÛŒÙ† Ù…ØªØ®ØµØµ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ù…Ù†Ø¬Ø± Ø¨Ù‡ ØªØ®ØµØµÛŒ Ø´Ø¯Ù† Ùˆ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡ Ùˆ Ù¾Ø±Ø§Ú©Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

Ù†ØªÛŒØ¬Ù‡ Ø§ÛŒÙ† ØªØ±Ú©ÛŒØ¨ØŒ ÛŒÚ© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø¨Ø³ÛŒØ§Ø± ØªØ·Ø¨ÛŒÙ‚â€ŒÙ¾Ø°ÛŒØ± Ø§Ø³Øª Ú©Ù‡ Ø¹Ù…Ù‚ Ùˆ Ø¹Ø±Ø¶ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ø·ÙˆÙ„ Ø¢Ù…ÙˆØ²Ø´ Ù‡Ø±Ø³ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ ØªÙˆØ³Ø· Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù… Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø§Ú©Ù†Ø¯Ú¯ÛŒØŒ ØªÙˆØ²ÛŒØ¹ Ø¨Ø§Ø± Ùˆ Ø§Ú©ØªØ´Ø§Ù Ú©Ù†ØªØ±Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

### Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ
- **Ø¹Ù…Ù‚ Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ©**: Ø¨Ù‡ Ø·ÙˆØ± Ø®ÙˆØ¯Ú©Ø§Ø± ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ú©Ø¯Ø§Ù… Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ Ø±Ø§ Ø±Ø¯ Ú©Ù†Ø¯.
- **Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ MoE**: ØªÙ†Ù‡Ø§ ÛŒÚ© Ú¯Ø±ÙˆÙ‡ ØªØ®ØµØµÛŒ Ø§Ø² Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø± Ù‡Ø± Ù„Ø§ÛŒÙ‡ ÙØ¹Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
- **Ù…Ø³ÛŒØ±ÛŒØ§Ø¨â€ŒÙ‡Ø§ÛŒ Two-Tower**: ÛŒÚ© Ù…Ú©Ø§Ù†ÛŒØ²Ù… Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…ØªØ®ØµØµ.
- **Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ**: Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø®ØªÛŒØ§Ø±ÛŒ Ø§Ø² ØªÙˆØ²ÛŒØ¹ Ø¨Ø§Ø± (load balancing) Ùˆ Ù¾Ø§Ø¯Ø§Ø´ Ø§Ù†ØªØ±ÙˆÙ¾ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ¶Ù…ÛŒÙ† Ø¢Ù…ÙˆØ²Ø´ Ù¾Ø§ÛŒØ¯Ø§Ø±.
- **API Ø³Ø§Ø¯Ù‡**: Ø³Ø§Ø®Øª Ùˆ ØªÙ†Ø¸ÛŒÙ… Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø¨Ù‡ Ø±Ø§Ø­ØªÛŒ Ø¨Ø§ Ú©Ù„Ø§Ø³ `SparrowConfig`.

### Ù†ØµØ¨
```bash
pip install sparrow-mlp
```
### Ø´Ø±ÙˆØ¹ Ø³Ø±ÛŒØ¹
Ø¯Ø± Ø¨Ø§Ù„Ø§ ÛŒÚ© Ù…Ø«Ø§Ù„ Ú©Ø§Ù…Ù„ Ø§Ø² Ø¢Ù…ÙˆØ²Ø´ `DynamicMLP` Ø±ÙˆÛŒ Ø¯ÛŒØªØ§Ø³Øª Ú¯Ù„ Ø²Ù†Ø¨Ù‚ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² `SparrowConfig` Ø¢Ù…Ø¯Ù‡ Ø§Ø³Øª.